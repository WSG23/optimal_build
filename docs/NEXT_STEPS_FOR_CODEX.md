# NEXT STEPS GUIDE FOR CODEX (AND OTHER AI AGENTS)

> **âš ï¸ IMPORTANT:** This is a **DECISION GUIDE**, not a status tracker.
>
> **For current project status:** See [feature_delivery_plan_v2.md](feature_delivery_plan_v2.md) â€” start with the "ğŸ“Š Current Progress Snapshot" section
>
> **Last Updated:** 2025-10-11

---

## ğŸ¯ Quick Start (20 Minutes)

**New to this project? Follow these steps:**

### Step 1: Check Current Status (5 min)
â†’ Read [feature_delivery_plan_v2.md](feature_delivery_plan_v2.md), beginning with the "ğŸ“Š Current Progress Snapshot" section

Look for:
- âœ… What's complete
- â¸ï¸ What's in progress
- âŒ What's not started

### Step 2: Check for Known Issues (5 min)
â†’ Read [TESTING_KNOWN_ISSUES.md](../TESTING_KNOWN_ISSUES.md)

This prevents you from:
- âŒ Investigating known test harness issues
- âŒ Trying to fix documented limitations
- âŒ Wasting time on non-problems

**âš ï¸ IMPORTANT:** When you fix a known issue or discover a new test limitation, you MUST update this file. See step 7b below for the workflow.

### Step 3: Understand the Codebase (5 min)
â†’ Read [FEATURES.md](../FEATURES.md) (just scan headers)
â†’ Read [CODING_RULES.md](../CODING_RULES.md) (reference while coding)

### Step 4: Choose Your Task (5 min)
â†’ Use the decision tree below

### Step 5: After Implementation - ALWAYS Suggest Tests âš ï¸
â†’ When you complete ANY feature, you MUST provide test commands to the user
â†’ See "MANDATORY TESTING CHECKLIST" section below (step 7)

**Non-negotiable:** Every implementation ends with test suggestions, even if you can't run them yourself.

---

## ğŸŒ³ Decision Tree: What Should I Build?

### Question 1: Is there a âŒ NOT STARTED task in current phase?

**Check:** [feature_delivery_plan_v2.md](feature_delivery_plan_v2.md) â€” use the current phase callout in "ğŸ“Š Current Progress Snapshot"

**If YES:** Build that task (skip to "How to Start")

**If NO:** Go to Question 2

---

### Question 2: Is current phase blocked?

**Common blockers:**
- "Waiting for API credentials"
- "Waiting for user validation"
- "Depends on Phase X completion"

**If BLOCKED:** Go to Question 3

**If NOT BLOCKED:** Something's wrong - ask the user

---

### Question 3: What's the next unblocked phase?

**Check:** [feature_delivery_plan_v2.md](feature_delivery_plan_v2.md) â€” review the "Phase 1D: Business Performance Management" section and the phases that follow

**Look for:**
- Phase marked âŒ NOT STARTED
- No dependency blockers
- No external requirements

**Found one?** Build that (skip to "How to Start")

**All phases blocked?** Ask the user for direction

---

## ğŸ“š How to Start a New Phase

### Before Writing Code:

**1. Read the phase requirements**
- Location: [feature_delivery_plan_v2.md](feature_delivery_plan_v2.md)
- Find your phase section
- Note: Requirements, Acceptance Criteria, Estimated Effort

**2. Check FEATURES.md for details**
- The plan will reference section headers (e.g., "Phase 1D: Business Performance Management")
- Read those sections in [FEATURES.md](../FEATURES.md)
- This is the detailed specification

**3. Verify dependencies**
- Check "Dependencies" section in the phase
- Ensure prerequisite phases are âœ… COMPLETE
- If blocked, choose a different phase

### While Building:

**4. Follow CODING_RULES.md**
- [CODING_RULES.md](../CODING_RULES.md) has:
  - Migration patterns
  - Async/await standards
  - Import ordering rules
  - Singapore compliance requirements

**5. Write tests as you go**
- Backend tests: Always write and make them pass
- Frontend tests: Write them, but known JSDOM timing issues exist
- Document test status in commit message

**6. Check for existing patterns**
- Look at completed phases for patterns
- Reuse service/API/frontend structures
- Keep consistency

### After Completing Work:

**âš ï¸ MANDATORY TESTING CHECKLIST**

**7. Run tests and provide user with test commands** (CRITICAL - DO NOT SKIP)

When you complete ANY feature implementation, you MUST:

âœ… **a) Run backend tests locally (if you can):**
```bash
python -m pytest backend/tests/test_[your_feature].py -v
```

âœ… **b) ALWAYS suggest test commands to the user:**
```markdown
"I've completed [feature name]. Please run these tests locally to verify:

Backend tests:
python -m pytest backend/tests/test_services/test_[feature].py -v
python -m pytest backend/tests/test_api/test_[feature].py -v

Frontend tests (if applicable):
npm run lint
NODE_ENV=test node --test --import tsx --import ./scripts/test-bootstrap.mjs frontend/src/pages/__tests__/[Feature]Page.test.tsx

Manual UI testing:
1. Start the server: make dev
2. Navigate to: http://localhost:4400/#/[route]
3. Test: [specific actions to verify]"
```

âœ… **c) Document test results in your message:**
- If tests pass: "âœ… All tests passing"
- If tests skip: "âš ï¸ Test skipped because [reason]"
- If tests fail: "âŒ Tests failing - needs debugging"

**Why this matters:**
- User needs to verify your work locally
- Test results inform commit message
- Prevents shipping broken code
- Gives user confidence in the implementation

---

**7b. Update TESTING_KNOWN_ISSUES.md if needed** (CRITICAL)

When you encounter test issues, you MUST check and update TESTING_KNOWN_ISSUES.md:

âœ… **If you FIX a known issue:**
- Move it from "Active Issues" to "Resolved Issues"
- Add resolution date, who fixed it, and how
- Update the "Files Changed" section
- Example: "Backend: API Tests Skipped on Python 3.9 - RESOLVED"

âœ… **If you DISCOVER a test harness limitation (not a real bug):**
- Check if it's already in "Active Issues"
- If NEW, propose adding it to the user
- Include: symptom, root cause, impact, workaround
- Example: "Frontend tests timeout but app works correctly"

âœ… **If tests fail due to REAL bugs:**
- Do NOT add to TESTING_KNOWN_ISSUES.md
- Fix the bug instead

**Why this matters:**
- Prevents future AI agents from wasting time on known issues
- Documents test infrastructure limitations vs real bugs
- Builds institutional knowledge across sessions

---

**8. Update feature_delivery_plan_v2.md** (CRITICAL)
```diff
- ### Phase X: Feature Name âŒ NOT STARTED
+ ### Phase X: Feature Name âœ… COMPLETE
+ **Status:** 100% - Completed [Month] 2025
+ **Test Status:** [Backend passing / Frontend status]
+ **Files Delivered:** [list key files]
```

**9. Commit with descriptive message**
```bash
git commit -m "Complete Phase X: Feature Name

- Delivered: [key features]
- Tests: Backend passing, Frontend [status]
- Files: [key files created/modified]

See feature_delivery_plan_v2.md for full details."
```

---

## ğŸš« What NOT to Do

### âŒ Don't Duplicate Status in This File

**WRONG:**
```markdown
Phase 1B: âœ… COMPLETE
Phase 1C: âœ… COMPLETE
Phase 1D: âŒ NOT STARTED â† Do this next
```

**This creates two sources of truth and they WILL get out of sync.**

**RIGHT:**
```markdown
For current status, see the "ğŸ“Š Current Progress Snapshot" section in feature_delivery_plan_v2.md
```

---

### âŒ Don't Skip the Delivery Plan

**WRONG:** Just ask user "what should I build?"

**RIGHT:**
1. Check delivery plan for current phase
2. If unclear or blocked, THEN ask user

---

### âŒ Don't Start Phase 2 Before Phase 1 Done

**Check:** [feature_delivery_plan_v2.md](feature_delivery_plan_v2.md) â€” look at the "Phase 1 Completion Gate" checklist

**Phase 1 Completion Gate:**
- All 6 Agent tools implemented
- User validation complete
- Feedback incorporated

**Don't jump ahead** - the phases have dependencies.

---

## ğŸ¯ Common Scenarios

### Scenario 1: "I'm starting fresh on this project"

**Do this:**
1. Read the "ğŸ“Š Current Progress Snapshot" section in feature_delivery_plan_v2.md (5 min)
2. Read TESTING_KNOWN_ISSUES.md (5 min)
3. Find first âŒ NOT STARTED task without blockers
4. Read that phase's requirements
5. Start building

**Time to first code:** ~20 minutes

---

### Scenario 2: "Previous AI agent just finished Phase X"

**Do this:**
1. Verify Phase X shows âœ… COMPLETE in feature_delivery_plan_v2.md
2. Look for next âŒ NOT STARTED phase
3. Check for blockers
4. If blocked, find next unblocked phase
5. Start building

**Time to continue:** ~10 minutes

---

### Scenario 3: "Tests are failing - is this a bug?"

**Do this:**
1. Check TESTING_KNOWN_ISSUES.md first
2. Is it listed there? â†’ Not a bug, skip it
3. Not listed? â†’ Investigate as real bug
4. If new issue found â†’ Document it (see workflow in TESTING_KNOWN_ISSUES.md)

---

### Scenario 4: "Phase is blocked on API credentials"

**Do this:**
1. Note the blocker in your message to user
2. Find next unblocked phase
3. Build that instead
4. Keep momentum going

**Don't:** Wait around for credentials

---

### Scenario 5: "User says 'continue where we left off'"

**Do this:**
1. Read the "ğŸ“Š Current Progress Snapshot" section in feature_delivery_plan_v2.md
2. Find the most recent âœ… COMPLETE phase
3. Look for next âŒ NOT STARTED
4. That's where you left off
5. Confirm with user if unclear

---

## ğŸ“Š Understanding Phase Dependencies

### Phase Order (from delivery plan):

```
Phase 1: Agent Foundation (A â†’ B â†’ C â†’ D)
  â”œâ”€ 1A: GPS Capture âœ… No dependencies
  â”œâ”€ 1B: Advisory âš ï¸ Can start (uses 1A data)
  â”œâ”€ 1C: Integrations âš ï¸ Can start (parallel with 1B)
  â””â”€ 1D: Performance âš ï¸ Uses data from 1B + 1C

Phase 2: Developer Foundation (depends on Phase 1 complete)
  â””â”€ Can't start until Phase 1 validated

Phase 3+: Later phases (depend on Phase 2)
```

**Key rule:** Don't skip ahead. Validate before moving to next major phase.

---

## ğŸ¤ When to Ask the User

### ASK when:
- âœ… Current phase is blocked and ALL phases are blocked
- âœ… Requirements are ambiguous or contradictory
- âœ… You need external credentials/API access
- âœ… Major architectural decision needed
- âœ… Validation results change priorities

### DON'T ASK when:
- âŒ Status is clear in feature_delivery_plan_v2.md
- âŒ Test failure is in TESTING_KNOWN_ISSUES.md
- âŒ Coding pattern exists in codebase
- âŒ Answer is in FEATURES.md or CODING_RULES.md

**Be autonomous** - most answers are in the docs.

---

## ğŸ”„ Update Workflow

### When You Complete a Phase:

**1. Update feature_delivery_plan_v2.md (Required)**
- Change âŒ NOT STARTED â†’ âœ… COMPLETE
- Add completion date
- Add test status
- List key files delivered

**2. Do NOT Update This File**
- This file is a guide, not a status tracker
- Status lives in feature_delivery_plan_v2.md only

**3. Update TESTING_KNOWN_ISSUES.md (If Applicable)**
- Add new issues to "Active Issues"
- Move resolved issues to "Resolved Issues"
- Follow the workflow in that document

---

## ğŸ“ Quick Reference

| Question | Answer |
|----------|--------|
| **"What's the current status?"** | [feature_delivery_plan_v2.md](feature_delivery_plan_v2.md) â€” see "ğŸ“Š Current Progress Snapshot" |
| **"What should I build next?"** | Use decision tree above |
| **"Are there known test issues?"** | [TESTING_KNOWN_ISSUES.md](../TESTING_KNOWN_ISSUES.md) |
| **"What are the requirements?"** | [FEATURES.md](../FEATURES.md) + phase section in delivery plan |
| **"How should I write code?"** | [CODING_RULES.md](../CODING_RULES.md) |
| **"How do I update docs?"** | See "Update Workflow" above |

---

## ğŸ¯ Success Checklist

Before saying "I'm done with Phase X":

- [ ] âœ… Feature works as specified in FEATURES.md
- [ ] âœ… Backend tests written and passing
- [ ] âœ… Frontend tests written (even if timing issues exist)
- [ ] âœ… Code follows CODING_RULES.md
- [ ] âœ… feature_delivery_plan_v2.md updated (status â†’ âœ… COMPLETE)
- [ ] âœ… Commit message describes what was delivered
- [ ] âœ… Known issues documented (if any)

---

## ğŸš€ Ready to Start?

**Go to:** the "ğŸ“Š Current Progress Snapshot" section in [feature_delivery_plan_v2.md](feature_delivery_plan_v2.md)

Find your next task and start building! ğŸ’ª
