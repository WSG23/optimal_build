# NEXT STEPS GUIDE FOR CODEX (AND OTHER AI AGENTS)

> **‚ö†Ô∏è IMPORTANT:** This is a **DECISION GUIDE**, not a status tracker.
>
> **For current project status:** See [feature_delivery_plan_v2.md](feature_delivery_plan_v2.md) ‚Äî start with the "üìä Current Progress Snapshot" section
>
> **Last Updated:** 2025-10-11

---

## üéØ Quick Start (20 Minutes)

**New to this project? Follow these steps:**

### Step 0: Understand Frontend UI Status (3 min) üö® CRITICAL
‚Üí Read [UI_STATUS.md](../UI_STATUS.md) FIRST

**This prevents massive waste of time:**
- ‚ùå Polishing test UI that will be replaced
- ‚ùå Treating test harness as production code
- ‚ùå Adding offline fallback / professional error handling to temporary scaffolding
- ‚ùå Applying B2B professional tool standards to development tools

**CRITICAL FACT:** All frontend UI was created by AI agents as test harnesses. Product owner was NOT involved in UI design. DO NOT polish it.

**Before touching ANY frontend file, read UI_STATUS.md.**

### Step 1: Check Current Status (5 min)
‚Üí Read [feature_delivery_plan_v2.md](feature_delivery_plan_v2.md), beginning with the "üìä Current Progress Snapshot" section

Look for:
- ‚úÖ What's complete
- ‚è∏Ô∏è What's in progress
- ‚ùå What's not started

### Step 2: Check for Known Issues (5 min)
‚Üí Read [TESTING_KNOWN_ISSUES.md](../TESTING_KNOWN_ISSUES.md)

This prevents you from:
- ‚ùå Investigating known test harness issues
- ‚ùå Trying to fix documented limitations
- ‚ùå Wasting time on non-problems

**‚ö†Ô∏è IMPORTANT:** When you fix a known issue or discover a new test limitation, you MUST update this file. See step 7b below for the workflow.

### Step 3: Understand the Codebase (5 min)
‚Üí Read [FEATURES.md](../FEATURES.md) (just scan headers)
‚Üí Read [CODING_RULES.md](../CODING_RULES.md) (reference while coding)

### Step 4: Choose Your Task (5 min)
‚Üí Use the decision tree below

### Step 5: After Implementation - ALWAYS Suggest Tests ‚ö†Ô∏è
‚Üí When you complete ANY feature, you MUST provide test commands to the user
‚Üí See "MANDATORY TESTING CHECKLIST" section below (step 7)

**Non-negotiable:** Every implementation ends with test suggestions, even if you can't run them yourself.

---

## üå≥ Decision Tree: What Should I Build?

### Question 1: Is there a ‚ùå NOT STARTED task in current phase?

**Check:** [feature_delivery_plan_v2.md](feature_delivery_plan_v2.md) ‚Äî use the current phase callout in "üìä Current Progress Snapshot"

**If YES:** Build that task (skip to "How to Start")

**If NO:** Go to Question 2

---

### Question 2: Is current phase blocked?

**Common blockers:**
- "Waiting for API credentials"
- "Waiting for user validation"
- "Depends on Phase X completion"

**If BLOCKED:** Go to Question 3

**If NOT BLOCKED:** Something's wrong - ask the user

---

### Question 3: What's the next unblocked phase?

**Check:** [feature_delivery_plan_v2.md](feature_delivery_plan_v2.md) ‚Äî review the "Phase 1D: Business Performance Management" section and the phases that follow

**Look for:**
- Phase marked ‚ùå NOT STARTED
- No dependency blockers
- No external requirements

**Found one?** Build that (skip to "How to Start")

**All phases blocked?** Ask the user for direction

---

## üìö How to Start a New Phase

### Before Writing Code:

**1. Read the phase requirements**
- Location: [feature_delivery_plan_v2.md](feature_delivery_plan_v2.md)
- Find your phase section
- Note: Requirements, Acceptance Criteria, Estimated Effort

**2. Check FEATURES.md for details**
- The plan will reference section headers (e.g., "Phase 1D: Business Performance Management")
- Read those sections in [FEATURES.md](../FEATURES.md)
- This is the detailed specification

**3. Verify dependencies**
- Check "Dependencies" section in the phase
- Ensure prerequisite phases are ‚úÖ COMPLETE
- If blocked, choose a different phase

### While Building:

**4. Follow CODING_RULES.md**
- [CODING_RULES.md](../CODING_RULES.md) has:
  - Migration patterns
  - Async/await standards
  - Import ordering rules
  - Singapore compliance requirements

**5. Write tests as you go**
- Backend tests: Always write and make them pass
- Frontend tests: Write them, but known JSDOM timing issues exist
- Document test status in commit message

**6. Check for existing patterns**
- Look at completed phases for patterns
- Reuse service/API/frontend structures
- Keep consistency

### After Completing Work:

**‚ö†Ô∏è MANDATORY TESTING CHECKLIST**

**7. Run tests and provide user with test commands** (CRITICAL - DO NOT SKIP)

When you complete ANY feature implementation, you MUST:

‚úÖ **a) Run backend tests locally (if you can):**
```bash
python -m pytest backend/tests/test_[your_feature].py -v
```

‚úÖ **b) ALWAYS suggest test commands to the user:**
```markdown
"I've completed [feature name]. Please run these tests locally to verify:

Backend tests:
python -m pytest backend/tests/test_services/test_[feature].py -v
python -m pytest backend/tests/test_api/test_[feature].py -v

Frontend tests (if applicable):
npm run lint
NODE_ENV=test node --test --import tsx --import ./scripts/test-bootstrap.mjs frontend/src/pages/__tests__/[Feature]Page.test.tsx

Manual UI testing:
1. Start the server: make dev
2. Navigate to: http://localhost:4400/#/[route]
3. Test: [specific actions to verify]"
```

‚úÖ **c) Document test results in your message:**
- If tests pass: "‚úÖ All tests passing"
- If tests skip: "‚ö†Ô∏è Test skipped because [reason]"
- If tests fail: "‚ùå Tests failing - needs debugging"

**Why this matters:**
- User needs to verify your work locally
- Test results inform commit message
- Prevents shipping broken code
- Gives user confidence in the implementation

---

**7b. Update TESTING_KNOWN_ISSUES.md if needed** (CRITICAL)

When you encounter test issues, you MUST check and update TESTING_KNOWN_ISSUES.md:

‚úÖ **If you FIX a known issue:**
- Move it from "Active Issues" to "Resolved Issues"
- Add resolution date, who fixed it, and how
- Update the "Files Changed" section
- Example: "Backend: API Tests Skipped on Python 3.9 - RESOLVED"

‚úÖ **If you DISCOVER a test harness limitation (not a real bug):**
- Check if it's already in "Active Issues"
- If NEW, propose adding it to the user
- Include: symptom, root cause, impact, workaround
- Example: "Frontend tests timeout but app works correctly"

‚úÖ **If tests fail due to REAL bugs:**
- Do NOT add to TESTING_KNOWN_ISSUES.md
- Fix the bug instead

**Why this matters:**
- Prevents future AI agents from wasting time on known issues
- Documents test infrastructure limitations vs real bugs
- Builds institutional knowledge across sessions

---

**7c. Update feature_delivery_plan_v2.md BEFORE asking user to commit** ‚ö†Ô∏è BLOCKING STEP

üõë **STOP: Do not proceed to commit until you complete this step.**

When you finish implementing a feature/milestone, you MUST update the status document **in the same commit** as your code changes.

**Why this is mandatory:**
- `feature_delivery_plan_v2.md` is the **SINGLE SOURCE OF TRUTH** for project status
- Other AI agents in new chat windows rely on it to know what's done
- User relies on it to track progress across sessions
- Outdated status creates confusion and duplicate work
- This document will be referenced for 6-12 months during pre-launch development

**Required updates:**

‚úÖ **Step 1: Update "üìä Current Progress Snapshot" section (lines 7-59)**

Find your phase and update the percentage + remaining items.

**For Backend Work:**
```diff
Example for Phase 1D after completing ROI Analytics backend:

- **Phase 1D: Business Performance Management** - Backend 45%, UI 30%
+ **Phase 1D: Business Performance Management** - Backend 60%, UI 30%
- Backend: Deal Pipeline API ‚úÖ, Remaining: ROI Analytics, Commission Protection
+ Backend: Deal Pipeline API ‚úÖ, ROI Analytics ‚úÖ, Remaining: Commission Protection
```

**For UI/UX Work:**
```diff
Example for Phase 1D after completing Pipeline Kanban UI:

- **Phase 1D: Business Performance Management** - Backend 60%, UI 30%
+ **Phase 1D: Business Performance Management** - Backend 60%, UI 50%
- UI: Production shell ‚úÖ, Remaining: Pipeline Kanban, Analytics panels
+ UI: Production shell ‚úÖ, Pipeline Kanban ‚úÖ, Remaining: Analytics panels
```

**How to calculate percentage:**
- Count total milestones in phase (e.g., 3 milestones: Deal Pipeline, ROI Analytics, Commission Protection)
- Divide completed by total (e.g., 2/3 = 66%, round to 60-65%)
- **Track Backend and UI separately** - they progress independently

‚úÖ **Step 2: Update detailed phase section (find "Phase X:" heading)**

**For Backend Work:**
Add to "Delivered (Milestone MX)" section:
```diff
+ **Delivered (Milestone M4 - ROI Analytics):**
+ - ‚úÖ ROI metrics aggregation in performance snapshots
+ - ‚úÖ compute_project_roi() integration from app.core.metrics
+ - ‚úÖ Snapshot context derivation (pipeline metadata)
+ - ‚úÖ Tests: test_agent_performance.py passing (4/4)
```

**For UI/UX Work:**
Add to "UI/UX Status" section (comes after Test Status, before Requirements):
```diff
+ **UI/UX Status (Production Customer-Facing Interface):**
+
+ **Delivered:**
+ - ‚úÖ Pipeline Kanban board component
+ - ‚úÖ Deal detail drawer with timeline/commissions
+
+ **In Progress (YYYY-MM-DD):**
+ - üîÑ Analytics panel
+ - üîÑ ROI panel
+
+ **UI Files:**
+ - frontend/src/app/pages/business-performance/PipelineBoard.tsx
+ - frontend/src/app/pages/business-performance/DealDrawer.tsx
```

‚úÖ **Step 3: Stage the documentation with your code changes**
```bash
git add docs/feature_delivery_plan_v2.md
# This goes in the SAME commit as your feature code
```

‚úÖ **Step 4: Mention documentation update in commit message**
```bash
git commit -m "Add Phase 1D ROI Analytics

Backend changes:
- ROI metrics aggregation in performance snapshots
- compute_project_roi() integration
- Snapshot context derivation

Tests: 4/4 passing

Updated docs/feature_delivery_plan_v2.md:
- Phase 1D progress: 45% ‚Üí 60%
- Added ROI Analytics to delivered milestones
```

**When to update:**
- ‚úÖ After tests pass (Step 7a-7b complete)
- ‚úÖ Before asking user to commit
- ‚úÖ Every time you complete a milestone/feature
- ‚úÖ Even for partial progress (e.g., backend done, frontend pending)

**What if you're not sure what to write?**
- Look at completed phases (1A, 1B, 1C) for examples
- Match the formatting and structure
- Ask user: "I completed X. Should I update feature_delivery_plan_v2.md to show X% progress?"

---

**7d. Manual Testing (REQUIRED for UI/UX work)** ‚ö†Ô∏è BLOCKING STEP

üõë **For UI/UX work: Do not proceed to commit until user completes manual testing**

When implementing customer-facing UI components, you MUST provide a detailed manual test script.

**When manual testing is REQUIRED:**
- ‚úÖ ANY customer-facing UI component (buttons, forms, cards, lists, etc.)
- ‚úÖ New pages or navigation changes
- ‚úÖ Form submissions or data entry
- ‚úÖ Charts, visualizations, or dashboards
- ‚úÖ Modals, drawers, popovers, or overlays
- ‚úÖ Interactive widgets (drag-drop, filters, sorting, pagination)
- ‚úÖ Any visual changes (styling, layout, colors, spacing)

**When manual testing is OPTIONAL:**
- ‚ö†Ô∏è Backend-only changes (no UI impact)
- ‚ö†Ô∏è Documentation updates
- ‚ö†Ô∏è Configuration or infrastructure changes

**Template for Manual Test Script:**

Provide this to the user:
```markdown
## Manual Testing Required: [Component/Feature Name]

### Setup
1. Start server: `make dev`
2. Navigate to: `http://localhost:4400/#/[route]`
3. Test as user role: [Agent/Developer/Architect]
4. Prerequisites: [Any data setup needed]

### Test Scenarios

**1. Happy Path (Primary User Journey)**
- **Action:** [e.g., Click "Create Deal" button]
- **Expected:** [e.g., Modal opens with form fields for deal details]
- **Verify:**
  - [ ] UI renders correctly (no layout breaks)
  - [ ] All interactive elements work
  - [ ] Data submits successfully
  - [ ] Success feedback shown

**2. Empty State**
- **Action:** [e.g., Load page with no existing deals]
- **Expected:** [e.g., Empty state message with CTA to create first deal]
- **Verify:**
  - [ ] Message is clear and user-friendly
  - [ ] CTA button is prominent and actionable
  - [ ] No broken UI or console errors

**3. Error State**
- **Action:** [e.g., Submit form with invalid data / trigger API error]
- **Expected:** [e.g., Validation errors display inline OR error message shows]
- **Verify:**
  - [ ] Error messages are user-friendly (not technical jargon)
  - [ ] Errors point to specific fields/issues
  - [ ] User knows how to fix the problem
  - [ ] UI remains functional after error

**4. Loading State**
- **Action:** [e.g., Trigger API call (may need network throttling)]
- **Expected:** [e.g., Loading spinner/skeleton appears while fetching]
- **Verify:**
  - [ ] Loading indicator is visible
  - [ ] UI doesn't flash or jump when data loads
  - [ ] Layout is stable (no content shift)
  - [ ] User can't trigger duplicate actions

**5. Complete Interaction Flow**
- **Action:** [e.g., Create deal ‚Üí view in pipeline ‚Üí update stage ‚Üí see analytics]
- **Expected:** [e.g., User completes entire workflow without confusion]
- **Verify:**
  - [ ] Navigation between steps is clear
  - [ ] User feedback at each step
  - [ ] No dead ends or confusing states
  - [ ] User achieves their goal successfully

**6. Edge Cases**
- **Long text:** [e.g., Deal name with 100+ characters]
  - [ ] Text truncates gracefully with ellipsis or wraps properly
- **Large numbers:** [e.g., Values over $1,000,000,000]
  - [ ] Numbers format correctly with commas/currency
- **Missing optional data:** [e.g., Deal without description]
  - [ ] UI handles gracefully (shows "‚Äî" or hides field)
- **Slow network:** [Throttle to Fast 3G in DevTools]
  - [ ] Loading states work correctly
  - [ ] No timeouts or hanging requests
- **Browser compatibility:** [Test in Chrome, Safari, Firefox]
  - [ ] UI works consistently across browsers

**7. Visual Quality**
- [ ] No alignment issues (elements properly aligned)
- [ ] Consistent spacing (padding/margins follow design system)
- [ ] Colors match design (no random colors)
- [ ] Typography correct (font sizes, weights, line heights)
- [ ] Icons render properly (no broken images)
- [ ] Responsive design works (test mobile viewport 375px width)

**8. Accessibility**
- [ ] **Keyboard navigation:** Tab through all interactive elements
- [ ] **Focus indicators:** Clear visual focus on active element
- [ ] **Screen reader:** Test with VoiceOver (Mac) or NVDA (Windows)
  - [ ] All buttons/links have descriptive labels
  - [ ] Form fields have associated labels
  - [ ] Error messages are announced
- [ ] **Color contrast:** Text readable (use browser DevTools)

---

### Test Results

Please complete the above tests and reply:

**"‚úÖ Manual testing complete - all scenarios passing"**

OR if issues found:

**"‚ùå Manual testing found issues: [describe problems]"**

---

**I will wait for your test confirmation before proceeding to commit.**
```

**Example for Phase 1D Pipeline Kanban:**
```markdown
## Manual Testing Required: Pipeline Kanban Board

### Setup
1. Start server: `make dev`
2. Navigate to: `http://localhost:4400/#/app/performance`
3. Test as: Agent Team Lead
4. Prerequisites: Ensure backend has test deals in database

### Test Scenarios

**1. Happy Path: View and Move Deals**
- **Action:** View pipeline board with deals in various stages
- **Expected:** Kanban columns show deals grouped by stage
- **Verify:**
  - [ ] All deals render in correct stage columns
  - [ ] Deal cards show: title, asset type, value, confidence %
  - [ ] Can drag deal card to different stage
  - [ ] Card updates position and backend updates stage

**2. Empty State**
- **Action:** View pipeline with no deals (clear DB or filter to empty result)
- **Expected:** Empty state message: "No deals yet. Create your first deal to get started."
- **Verify:**
  - [ ] Message is centered and clear
  - [ ] "Create Deal" CTA button is prominent
  - [ ] No broken Kanban layout

**3. Error State**
- **Action:** Disconnect network, try to move deal to new stage
- **Expected:** Error toast/message: "Failed to update deal stage. Please try again."
- **Verify:**
  - [ ] Error message appears
  - [ ] Deal reverts to original position
  - [ ] User can retry action

**4. Loading State**
- **Action:** Load page (throttle network to Fast 3G)
- **Expected:** Loading skeletons appear in each column
- **Verify:**
  - [ ] Skeleton cards visible while loading
  - [ ] No layout shift when real data loads
  - [ ] Smooth transition from skeleton to cards

**5. Complete Flow: Create ‚Üí Move ‚Üí View**
- **Action:** Create new deal ‚Üí appears in "Lead captured" ‚Üí drag to "Qualified" ‚Üí view timeline
- **Expected:** Deal moves through pipeline and timeline shows stage history
- **Verify:**
  - [ ] New deal appears immediately in correct column
  - [ ] Drag-and-drop feels smooth
  - [ ] Timeline shows stage transition with timestamp
  - [ ] Audit metadata visible (hash, changed by)

**6. Edge Cases**
- **Long deal name:** "Very Long Commercial Property Name That Exceeds Normal Length at Marina Bay Waterfront District"
  - [ ] Truncates with ellipsis, full name in tooltip
- **Large value:** $1,234,567,890
  - [ ] Formats as "$1.2B" or "$1,234,567,890" with commas
- **100+ deals:** Populate DB with many deals
  - [ ] Performance acceptable (no lag when scrolling)
  - [ ] Columns scroll independently if needed

**7. Visual Quality**
- [ ] Card spacing consistent within columns
- [ ] Column widths equal
- [ ] Deal cards have subtle shadow/border
- [ ] Confidence % shows as colored badge (green high, yellow medium, red low)
- [ ] Asset type icons render correctly

**8. Accessibility**
- [ ] Tab to each deal card
- [ ] Enter key opens deal details
- [ ] Arrow keys navigate between cards
- [ ] Screen reader announces: "Deal card: [name], [stage], [value]"

Please test and confirm: ‚úÖ All scenarios passing
```

**Why this step is critical:**
- Automated tests don't catch visual bugs (alignment, colors, spacing)
- UX flow issues require human judgment (is it confusing? intuitive?)
- Accessibility problems need manual verification (keyboard nav, screen reader)
- User experience quality depends on manual testing
- Frontend tests don't validate actual browser rendering
- Edge cases often slip through without manual verification

**What happens next:**
1. You provide test script (above template)
2. User completes manual testing
3. User confirms: "‚úÖ All scenarios passing" OR reports issues
4. If passing ‚Üí proceed to Step 8 (commit)
5. If issues ‚Üí fix them, ask user to retest

---

**8. Commit with descriptive message** (after docs updated AND manual testing complete)
```bash
git commit -m "Complete Phase X Milestone: Feature Name

Backend/Frontend changes:
- [Key feature 1]
- [Key feature 2]

Tests: [Backend X/X passing, Frontend status]
Files: [Key files created/modified]

Updated feature_delivery_plan_v2.md:
- Phase X progress: Y% ‚Üí Z%
- Added [Feature Name] to delivered milestones

See feature_delivery_plan_v2.md for full details."
```

---

## üö´ What NOT to Do

### ‚ùå Don't Duplicate Status in This File

**WRONG:**
```markdown
Phase 1B: ‚úÖ COMPLETE
Phase 1C: ‚úÖ COMPLETE
Phase 1D: ‚ùå NOT STARTED ‚Üê Do this next
```

**This creates two sources of truth and they WILL get out of sync.**

**RIGHT:**
```markdown
For current status, see the "üìä Current Progress Snapshot" section in feature_delivery_plan_v2.md
```

---

### ‚ùå Don't Skip the Delivery Plan

**WRONG:** Just ask user "what should I build?"

**RIGHT:**
1. Check delivery plan for current phase
2. If unclear or blocked, THEN ask user

---

### ‚ùå Don't Start Phase 2 Before Phase 1 Done

**Check:** [feature_delivery_plan_v2.md](feature_delivery_plan_v2.md) ‚Äî look at the "Phase 1 Completion Gate" checklist

**Phase 1 Completion Gate:**
- All 6 Agent tools implemented
- User validation complete
- Feedback incorporated

**Don't jump ahead** - the phases have dependencies.

---

## üéØ Common Scenarios

### Scenario 1: "I'm starting fresh on this project"

**Do this:**
1. Read the "üìä Current Progress Snapshot" section in feature_delivery_plan_v2.md (5 min)
2. Read TESTING_KNOWN_ISSUES.md (5 min)
3. Find first ‚ùå NOT STARTED task without blockers
4. Read that phase's requirements
5. Start building

**Time to first code:** ~20 minutes

---

### Scenario 2: "Previous AI agent just finished Phase X"

**Do this:**
1. Verify Phase X shows ‚úÖ COMPLETE in feature_delivery_plan_v2.md
2. Look for next ‚ùå NOT STARTED phase
3. Check for blockers
4. If blocked, find next unblocked phase
5. Start building

**Time to continue:** ~10 minutes

---

### Scenario 3: "Tests are failing - is this a bug?"

**Do this:**
1. Check TESTING_KNOWN_ISSUES.md first
2. Is it listed there? ‚Üí Not a bug, skip it
3. Not listed? ‚Üí Investigate as real bug
4. If new issue found ‚Üí Document it (see workflow in TESTING_KNOWN_ISSUES.md)

---

### Scenario 4: "Phase is blocked on API credentials"

**Do this:**
1. Note the blocker in your message to user
2. Find next unblocked phase
3. Build that instead
4. Keep momentum going

**Don't:** Wait around for credentials

---

### Scenario 5: "User says 'continue where we left off'"

**Do this:**
1. Read the "üìä Current Progress Snapshot" section in feature_delivery_plan_v2.md
2. Find the most recent ‚úÖ COMPLETE phase
3. Look for next ‚ùå NOT STARTED
4. That's where you left off
5. Confirm with user if unclear

---

## üìä Understanding Phase Dependencies

### Phase Order (from delivery plan):

```
Phase 1: Agent Foundation (A ‚Üí B ‚Üí C ‚Üí D)
  ‚îú‚îÄ 1A: GPS Capture ‚úÖ No dependencies
  ‚îú‚îÄ 1B: Advisory ‚ö†Ô∏è Can start (uses 1A data)
  ‚îú‚îÄ 1C: Integrations ‚ö†Ô∏è Can start (parallel with 1B)
  ‚îî‚îÄ 1D: Performance ‚ö†Ô∏è Uses data from 1B + 1C

Phase 2: Developer Foundation (depends on Phase 1 complete)
  ‚îî‚îÄ Can't start until Phase 1 validated

Phase 3+: Later phases (depend on Phase 2)
```

**Key rule:** Don't skip ahead. Validate before moving to next major phase.

---

## ü§ù When to Ask the User

### ASK when:
- ‚úÖ Current phase is blocked and ALL phases are blocked
- ‚úÖ Requirements are ambiguous or contradictory
- ‚úÖ You need external credentials/API access
- ‚úÖ Major architectural decision needed
- ‚úÖ Validation results change priorities

### DON'T ASK when:
- ‚ùå Status is clear in feature_delivery_plan_v2.md
- ‚ùå Test failure is in TESTING_KNOWN_ISSUES.md
- ‚ùå Coding pattern exists in codebase
- ‚ùå Answer is in FEATURES.md or CODING_RULES.md

**Be autonomous** - most answers are in the docs.

---

## üîÑ Update Workflow

### When You Complete a Phase:

**1. Update feature_delivery_plan_v2.md (Required)**
- Change ‚ùå NOT STARTED ‚Üí ‚úÖ COMPLETE
- Add completion date
- Add test status
- List key files delivered

**2. Do NOT Update This File**
- This file is a guide, not a status tracker
- Status lives in feature_delivery_plan_v2.md only

**3. Update TESTING_KNOWN_ISSUES.md (If Applicable)**
- Add new issues to "Active Issues"
- Move resolved issues to "Resolved Issues"
- Follow the workflow in that document

---

## üìû Quick Reference

| Question | Answer |
|----------|--------|
| **"What's the current status?"** | [feature_delivery_plan_v2.md](feature_delivery_plan_v2.md) ‚Äî see "üìä Current Progress Snapshot" |
| **"What should I build next?"** | Use decision tree above |
| **"Are there known test issues?"** | [TESTING_KNOWN_ISSUES.md](../TESTING_KNOWN_ISSUES.md) |
| **"What are the requirements?"** | [FEATURES.md](../FEATURES.md) + phase section in delivery plan |
| **"How should I write code?"** | [CODING_RULES.md](../CODING_RULES.md) |
| **"How do I update docs?"** | See "Update Workflow" above |

---

## üéØ Success Checklist

Before saying "I'm done with Phase X":

- [ ] ‚úÖ Feature works as specified in FEATURES.md
- [ ] ‚úÖ Backend tests written and passing
- [ ] ‚úÖ Frontend tests written (even if timing issues exist)
- [ ] ‚úÖ Code follows CODING_RULES.md
- [ ] ‚úÖ feature_delivery_plan_v2.md updated (status ‚Üí ‚úÖ COMPLETE)
- [ ] ‚úÖ Commit message describes what was delivered
- [ ] ‚úÖ Known issues documented (if any)

---

## üöÄ Ready to Start?

**Go to:** the "üìä Current Progress Snapshot" section in [feature_delivery_plan_v2.md](feature_delivery_plan_v2.md)

Find your next task and start building! üí™
