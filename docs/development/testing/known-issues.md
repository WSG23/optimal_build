# Known Testing Issues

This document tracks known issues with the test harness that do **not** indicate bugs in the application code.

## Purpose

This document helps both **human developers** and **AI agents** (Claude, Codex) distinguish between:
- ‚ùå Real bugs that need fixing
- ‚ö†Ô∏è Known test harness limitations that don't indicate broken functionality

**Why this matters for AI agents:**
AI agents have no memory between sessions and can't ask teammates about recurring issues. Without this documentation, an AI agent will waste time investigating and potentially "fixing" working code.

---

## Workflow for Adding New Issues

**When an AI agent discovers a potential "known issue":**
1. ü§ñ AI explains the issue and proposes documenting it
2. üë§ Human decides: "Document it" / "No, fix it" / "Document until Phase X"
3. ü§ñ AI adds/updates documentation with human approval
4. üìù Add "Documented by: [Human/AI name] on YYYY-MM-DD"

**When a human discovers a known issue:**
1. Add it to "Active Issues" below using the template
2. Add code comments pointing to this doc
3. Note discovery date and who found it

---

## Workflow for Resolving Issues

**When an AI agent fixes a known issue:**
1. ü§ñ AI fixes the issue and verifies tests pass
2. ü§ñ AI proposes documentation updates (see checklist below)
3. üë§ Human reviews: "Approved" / "Modify the documentation" / "Not actually fixed"
4. ü§ñ AI updates documentation with human approval

**When a human fixes a known issue:**
1. Fix the issue and verify tests pass
2. Follow the maintenance checklist below
3. Or ask an AI agent to update the documentation for you

**MAINTENANCE CHECKLIST** - What needs updating:
1. ‚úÖ Move the issue from "Active Issues" to "Resolved Issues" section
2. ‚úÖ Add resolution date, who fixed it, and fix description
3. ‚úÖ Remove or update code comments that reference this issue
4. ‚úÖ Search codebase for `TESTING_KNOWN_ISSUES.md` references and update them
5. ‚úÖ Update affected test files to remove workarounds
6. ‚úÖ Verify all tests now pass without workarounds

---

## Active Issues

### Dev Seeder: Postgres-Only Dependency

**Documented by:** Codex on 2025-10-13
**Affects:** `python -m backend.scripts.seed_properties_projects`, `make seed-data`

**Symptom:**
- Running the developer seeding scripts in the sandbox fails immediately with `PermissionError: [Errno 1] Operation not permitted` when the async engine tries to connect to `postgresql+asyncpg://localhost`.
- Forcing `SQLALCHEMY_DATABASE_URI=sqlite+aiosqlite://...` still fails because several tables (for example `agent_advisory_feedback`) declare `sqlalchemy.dialects.postgresql.UUID`, which SQLite cannot compile (`Compiler ... can't render element of type UUID`).

**Root Cause:**
The seeding scripts rely on Postgres-only column types and default to connecting to a running Postgres instance. The sandbox blocks that network call, and the schema currently cannot be created on SQLite due to the direct `postgresql.UUID` usage in older models.

**Impact:**
- ‚ùå We cannot execute `make seed-data` / `seed_properties_projects` inside the sandbox to verify database rows.
- ‚úÖ Runtime behaviour in real environments (Docker/Postgres) is unaffected.

**Workaround:**
Run the seeders via Docker (`make seed-data` with Docker Compose running) or another environment that provides Postgres. For local sandbox validation rely on code review or manual Postgres access; no lightweight SQLite fallback exists yet.

**Next Steps:**
Refactor the affected models to use the project-wide `app.models.base.UUID` type before attempting to support SQLite-based seeding.


### Frontend: React Testing Library Async Timing

**Documented by:** Claude on 2025-10-11
**Affects:** Phase 1B (Agent Advisory), Phase 1C (Listing Integrations)

**Symptom:**
Frontend tests fail with "Unable to find an element with the text..." even though the HTML dump clearly shows the content is rendered correctly in the DOM.

**Example:**
```
Unable to find an element with the text: /Linked accounts/i

Ignored nodes: comments, script, style
<body>
  <div class="integrations__panel">
    <h2>Linked accounts</h2>  <!-- ‚Üê Content is clearly present -->
    <p class="integrations__empty">No accounts linked yet.</p>
  </div>
</body>
```

**Root Cause:**
React state updates complete after `waitFor()` timeout in the JSDOM test environment. This is a test harness timing issue with how React Testing Library queries interact with JSDOM's async rendering cycle.

**Impact:**
- **Application functionality:** ‚úÖ Works correctly
- **Manual testing:** ‚úÖ Page renders and behaves correctly
- **Backend API tests:** ‚úÖ Pass successfully
- **Frontend unit tests:** ‚ùå Timeout finding rendered elements

**What NOT to do:**
- Do not modify application code to "fix" these test failures
- Do not remove or skip these tests entirely
- Do not assume the feature is broken

**What TO do:**
1. **Verify functionality manually** by running the app and testing the page
2. **Check backend tests pass** - if they do, the API layer is working
3. **Look at the HTML dump** in test output - if content is present, rendering works
4. **Document test status** in phase completion notes

**Tests affected:**
- `frontend/src/pages/__tests__/AgentAdvisoryPage.test.tsx` (Phase 1B)
- `frontend/src/pages/__tests__/AgentIntegrationsPage.test.tsx` (Phase 1C)

**Workarounds attempted:**
- ‚úÖ Increased `waitFor` timeout to 3000ms - still fails
- ‚úÖ Switched from `waitFor + getByText` to `findByText` - still fails
- ‚úÖ Added manual `setTimeout` delays - still fails
- ‚ùå Root fix pending: likely needs React Testing Library configuration or different query strategy

**Related issues:**
- Same pattern occurred in Phase 1B and Phase 1C
- Other page tests (e.g., AgentsGpsCapturePage) work because they don't have async `useEffect` data fetching on mount

**Future fix considerations:**
1. Configure React Testing Library with longer default timeouts for this project
2. Use `act()` wrapper more explicitly for async state updates
3. Mock the `useEffect` data fetching in tests
4. Switch to Playwright/Cypress for integration tests that don't have this JSDOM limitation

---

### SQLite vs PostgreSQL SQL Compatibility

**Documented by:** Claude on 2025-10-11
**Affects:** Backend tests using raw SQL

**Symptom:**
Raw SQL queries using PostgreSQL-specific syntax fail in tests.

**Example:**
```sql
-- ‚ùå Fails in SQLite tests
UPDATE listing_integration_accounts
SET expires_at = NOW() - INTERVAL '10 minutes'

-- ‚úÖ Works in SQLite
UPDATE listing_integration_accounts
SET expires_at = datetime('now', '-10 minutes')
```

**Solution:**
Always wrap raw SQL in `text()` and use SQLite-compatible syntax for tests:
```python
from sqlalchemy import text

await session.execute(
    text("UPDATE table SET expires_at = datetime('now', '-10 minutes')")
)
```

**Note:** Production uses PostgreSQL, but tests use in-memory SQLite for speed.

---

### Marketing Packs UI Requires Demo Seed Data

**Documented by:** Codex on 2025-10-13
**Affects:** Manual testing of the Marketing Packs UI in development

**Symptom:**
Clicking "Generate" on `http://127.0.0.1:4415/app/marketing` shows
```
Using offline preview pack. Backend generator unavailable. (No row was found when one was required)
```
and no PDF is generated.

**Root Cause:**
The Universal Site Pack generator depends on a property plus related market
intel rows (transactions, rental listings, yield benchmarks). The default
SQLite database shipped in `.devstack/app.db` only contains the basic fallback
property, so the generator raises a `NoResultFound` and the UI falls back to the
offline preview.

**Impact:**
- ‚úÖ Backend PDF endpoint works when demo data is present
- ‚ö†Ô∏è Frontend always falls back unless demo data is seeded
- ‚ö†Ô∏è Manual UI test cannot confirm real PDF generation without seeding

**Workaround:**
Seed the demo dataset into the local SQLite database before manual testing:

```bash
cd /Users/wakaekihara/GitHub/optimal_build
source .venv/bin/activate
SQLALCHEMY_DATABASE_URI="sqlite+aiosqlite:///./.devstack/app.db" \
  python -m backend.scripts.seed_market_demo
```

This prints the demo property UUID (e.g. `Market Demo Tower`). Restart uvicorn
and use that ID on the Marketing page. The pack generator will then create a
real PDF.

**What NOT to do:**
- Don‚Äôt modify the generator or UI; the issue is missing seed data.
- Don‚Äôt run `create_test_property.py` (it targets Postgres by default).

**Follow-up:** Coordinate with Claude to switch the default dev database to
PostgreSQL or ship seeded SQLite fixtures so the UI works out-of-the-box.

---

### Frontend: Node.js Test Runner JSDOM Environment Setup Issues

**Documented by:** Claude on 2025-10-19
**Affects:** `npm --prefix frontend run test` (10 out of 26 tests failing)

**Symptom:**
Multiple frontend component tests fail with environment setup errors when running via Node.js test runner:
```
TypeError: Cannot set property navigator of #<Object> which has only a getter
ReferenceError: window is not defined
TypeError: For queries bound to document.body a global document has to be available
```

**Failing Tests (10/26):**
- `FeasibilityWizard.test.tsx` - window is not defined
- `FinanceStackAndDrawdown.test.tsx` (2 tests) - navigator property error, document not available
- `AgentAdvisoryPage.test.tsx` - navigator property error
- `AgentIntegrationsPage.test.tsx` - navigator property error
- `AgentPerformancePage.test.tsx` - navigator property error
- `AgentsGpsCapturePage.test.tsx` - window is not defined
- `AdvancedIntelligence.test.tsx` (3 tests) - navigator property error, document not available

**Passing Tests (16/26):**
- All API-level tests (agents, buildable, finance) - ‚úÖ Passing
- CAD detection helpers - ‚úÖ Passing

**Root Cause:**
The Node.js test runner with JSDOM setup in `scripts/test-bootstrap.mjs` has incomplete browser environment mocking. Specifically:
1. `window` object not properly initialized for some tests
2. `navigator` property configured as read-only in some contexts
3. `document.body` not available for React Testing Library's `screen` queries

**Impact:**
- ‚ùå Cannot run full frontend test suite successfully (10/26 tests fail)
- ‚úÖ API-level tests and pure logic tests work fine
- ‚úÖ **Application code works correctly in production** - these are test environment issues, not code bugs
- ‚úÖ Component-specific tests for new features (e.g., SiteAcquisitionPage) don't exist yet, so this doesn't block new feature development

**Workaround:**
For feature development:
1. Run backend tests for backend changes
2. Run `npm --prefix frontend run lint` to catch syntax/style issues
3. Test manually in browser (`make dev` and navigate to component)
4. Accept that some pre-existing component tests will fail due to test setup issues

For CI/CD:
- Consider setting frontend test suite to "advisory" status until fixed
- Or configure CI to expect 10 known failures

**Next Steps:**
1. Fix `scripts/test-bootstrap.mjs` to properly initialize JSDOM environment
2. Add proper window/navigator/document mocks in test setup
3. Ensure all 26 tests pass
4. Consider switching to Vitest (has better JSDOM integration) or Jest (more mature test runner)

**Estimated Fix Effort:** 2-4 hours
- Investigate test-bootstrap.mjs and JSDOM setup
- Add proper mocks for window/navigator/document
- Update failing test files if needed
- Verify all 26 tests pass

---

## Resolved Issues

### Migration Audit: Legacy Downgrade Guards Missing - RESOLVED

**Resolution Date:** 2025-10-18
**Resolved By:** Claude (automated verification)
**Fix Description:** Migration files already had proper guards with `if_exists=True`. Added to `.coding-rules-exceptions.yml` to bypass audit script pattern matching.

**How it was resolved:**
1. Verified that all 3 migration files (`20250220_000012`, `20250220_000013`, `20251013_000014`) already had `if_exists=True` guards on all `op.drop_table()` and `op.drop_index()` calls
2. Files were already listed in `.coding-rules-exceptions.yml` under `rule_1_migrations`
3. Confirmed `scripts/audit_migrations.py` passes successfully
4. Issue was documentation lag - migrations were fixed but TESTING_KNOWN_ISSUES.md wasn't updated

**Files verified:**
- `backend/migrations/versions/20250220_000012_add_commission_tables.py` - All drops guarded
- `backend/migrations/versions/20250220_000013_add_performance_snapshots.py` - All drops guarded
- `backend/migrations/versions/20251013_000014_add_developer_due_diligence_checklists.py` - All drops guarded
- `.coding-rules-exceptions.yml` - Exceptions documented

**Test Results:**
```bash
python3 scripts/audit_migrations.py
# All migrations pass
```

**Original Issue:** Pre-commit hook `audit-migrations` failed because the script pattern-matches ANY `op.drop_*` calls, regardless of guards. The solution was to add well-guarded migrations to the exceptions list.

---

### Backend: API Tests Skipped on Python 3.9 - RESOLVED

**Resolution Date:** 2025-10-11
**Resolved By:** Claude (with user)
**Fix Description:** Upgraded to Python 3.13 and added proper dependency mocking for FastAPI tests

**How it was fixed:**
1. Upgraded Python from 3.9.6 to 3.13.8 using Homebrew
2. Created fresh virtual environment with Python 3.13
3. Added dependency overrides in test fixture for `require_reviewer`, `require_viewer`, and `get_optional_user`
4. Installed minimal dependencies: `python-jose[cryptography]`, `passlib[bcrypt]`, `httpx`, `pytest-asyncio`

**Files Changed:**
- `backend/tests/test_api/test_deals.py` - Added dependency mocking for auth and role checks
- `.venv/` - Recreated with Python 3.13.8

**Test Results:**
```bash
python -m pytest backend/tests/test_services/test_agent_deal_pipeline.py backend/tests/test_api/test_deals.py -v
# Result: 3 passed, 0 skipped ‚úÖ
```

**Original Issue:** API tests were skipped on Python 3.9 due to FastAPI dependency injection limitations

---

### PDF Rendering Dependencies Absent in Sandbox

**Documented by:** Codex on 2025-10-28
**Affects:** `backend/tests/test_services/test_universal_site_pack.py`, `backend/tests/test_integration/test_pdf_download_flow.py`

**Symptom:**
PDF-related tests crash with `ImportError` from WeasyPrint because the native Cairo/Pango/GDK libraries required by the renderer are not present in the sandbox.

**Root Cause:**
The sandbox environment does not ship the system-level dependencies that WeasyPrint needs (cairo, pango, gdk-pixbuf, libffi). Installing them requires Homebrew/apt access, which is blocked in this workspace.

**Impact:**
- ‚úÖ Application functionality works in environments where the dependencies are installed (Docker compose, developer laptops).
- ‚ùå Automated PDF tests cannot run inside the sandbox; the import fails before any assertions execute.

**Workaround:**
- Skip the affected tests with `@pytest.mark.skip(reason="PDF rendering dependencies (WeasyPrint/Cairo) are unavailable in the audit sandbox")`.
- When PDF behaviour needs verification, run the suite locally after installing the system packages (`brew install cairo pango gdk-pixbuf libffi`) or rely on manual QA exports.

**Next Steps:**
- When the sandbox grows native PDF support, remove the skip markers and move this entry to the Resolved section.
- Consider adding a lightweight mock renderer so the logic can be exercised even without the native stack.

---

### Example of AI Agent Resolution Workflow

Here's how it would work when an AI agent fixes an issue:

```
ü§ñ AI: "I've fixed the React Testing Library async timing issue. All tests now pass
       consistently. Here's what I changed:

       - Configured React Testing Library with 5000ms default timeout
       - Added proper `act()` wrappers around async state updates
       - Updated test setup in test-bootstrap.mjs

       Tests now pass:
       ‚úÖ frontend/src/pages/__tests__/AgentAdvisoryPage.test.tsx
       ‚úÖ frontend/src/pages/__tests__/AgentIntegrationsPage.test.tsx

       Should I update TESTING_KNOWN_ISSUES.md to move this to 'Resolved Issues'
       and remove the warning comments from the test files?"

üë§ You: "Yes, approved"

ü§ñ AI: *Updates documentation following the maintenance checklist*
```

---

### Template for Resolved Issues:
```
### [Issue Name] - RESOLVED

**Resolution Date:** YYYY-MM-DD
**Resolved By:** @username or AI agent name
**Fix Description:** Brief description of how it was fixed
**Files Changed:**
- `path/to/file.ts` - what changed
- `path/to/test.tsx` - what changed

**Original Issue:** [Brief description or link to git history]
```
